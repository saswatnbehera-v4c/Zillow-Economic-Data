# Databricks notebook source
# MAGIC %run ../../config/project_config

# COMMAND ----------

def create_bronze_views():
    # List of files from your Volume to expose as views
    # Excluding: metrics, fields_per_level, and DataDictionary as requested
    files_to_expose = [
        "Metro_time_series.csv",
        "CountyCrossWalk_Zillow.csv",
        "State_time_series.csv",
        "Neighborhood_time_series.csv",
        "cities_crosswalk.csv"
    ]

    for file_name in files_to_expose:
        # Generate a clean view name (e.g., v_county_time_series_chunk_3)
        clean_name = f"v_{file_name.split('.')[0].lower()}"
        file_path = f"{VOLUME_LANDING}/{file_name}"
        file_ext = file_name.split('.')[-1].lower()

        print(f"Creating view {clean_name} for {file_path}")

        # SQL for read_files
        # Note: read_files automatically handles schema inference for JSON/CSV
        spark.sql(f"""
            CREATE OR REPLACE VIEW {CATALOG_NAME}.{SCHEMA_BRONZE}.{clean_name}
            AS SELECT * FROM read_files('{file_path}', format => '{file_ext}')
        """)

# COMMAND ----------

create_bronze_views()

# COMMAND ----------

