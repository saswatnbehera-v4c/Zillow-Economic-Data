{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47bb0c5-12cd-4f46-b1a4-fa494629a209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/project_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0d2a28-035b-4447-8d0c-00776ed7bb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74821bde-90c6-4004-9168-36dbfe38aae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Table: Zillow.bronze.county_metrics\nRow Count: 259395\n✅ TEST 1 PASSED: Data exists.\n✅ TEST 2 PASSED: Audit columns exist.\n✅ TEST 3 PASSED: Audit columns are populated.\n✅ TEST 4 PASSED: Source file is csv.\n\n ALL UNIT TESTS PASSED SUCCESSFULLY.\n"
     ]
    }
   ],
   "source": [
    "target_table = f\"{CATALOG_NAME}.{SCHEMA_BRONZE}.county_metrics\"\n",
    "\n",
    "print(f\"Testing Table: {target_table}\")\n",
    "\n",
    "try:\n",
    "    df = spark.read.table(target_table)\n",
    "    \n",
    "    # --- TEST 1: DATA EXISTENCE ---\n",
    "    count = df.count()\n",
    "    print(f\"Row Count: {count}\")\n",
    "    assert count > 0, f\"❌ FAILURE: Table {target_table} is empty!\"\n",
    "    print(\"✅ TEST 1 PASSED: Data exists.\")\n",
    "\n",
    "    # --- TEST 2: AUDIT COLUMNS ---\n",
    "    columns = df.columns\n",
    "    assert \"load_dt\" in columns, \"❌ FAILURE: Column 'load_dt' is missing!\"\n",
    "    assert \"source_file\" in columns, \"❌ FAILURE: Column 'source_file' is missing!\"\n",
    "    print(\"✅ TEST 2 PASSED: Audit columns exist.\")\n",
    "\n",
    "    # --- TEST 3: AUDIT DATA QUALITY ---\n",
    "    null_audit_count = df.filter(col(\"load_dt\").isNull() | col(\"source_file\").isNull()).count()\n",
    "    assert null_audit_count == 0, f\"❌ FAILURE: {null_audit_count} rows have NULL audit values.\"\n",
    "    print(\"✅ TEST 3 PASSED: Audit columns are populated.\")\n",
    "\n",
    "    # --- TEST 4: Lineage Check ---\n",
    "    invalid_source_count = df.filter(~col(\"source_file\").contains(\".csv\")).count()\n",
    "    assert invalid_source_count == 0, f\"❌ FAIL: {invalid_source_count} rows have invalid source filenames.\"\n",
    "    print(\"✅ TEST 4 PASSED: Source file is csv.\")\n",
    "\n",
    "    print(\"\\n ALL UNIT TESTS PASSED SUCCESSFULLY.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ CRITICAL TEST FAILURE: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5519724822534308,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}