{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f22705b-0083-48a7-a653-5d67676452d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d126f2-6a40-4012-a293-2226c265af9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def detect_header(file_path, delimiter=','):\n",
    "    \"\"\"\n",
    "    Detect if CSV file has a header row\n",
    "    Returns True if header is detected, False otherwise\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read first few rows\n",
    "        sample = file.read(1024)\n",
    "        file.seek(0)\n",
    "        \n",
    "        # Use csv.Sniffer to detect header\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            has_header = sniffer.has_header(sample)\n",
    "            return has_header\n",
    "        except:\n",
    "            # Fallback: check if first row looks like header\n",
    "            reader = csv.reader(file, delimiter=delimiter)\n",
    "            first_row = next(reader, None)\n",
    "            second_row = next(reader, None)\n",
    "            \n",
    "            if not first_row or not second_row:\n",
    "                return False\n",
    "            \n",
    "            # Simple heuristic: if first row has non-numeric values and second row has more numeric values\n",
    "            first_numeric = sum(1 for cell in first_row if cell.replace('.', '').replace('-', '').isdigit())\n",
    "            second_numeric = sum(1 for cell in second_row if cell.replace('.', '').replace('-', '').isdigit())\n",
    "            \n",
    "            return first_numeric < second_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe6373c-0f9f-4c3c-b6c3-b757ce86756e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_rows(file_path, has_header=False, delimiter=','):\n",
    "    \"\"\"Count total rows in CSV file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter)\n",
    "        if has_header:\n",
    "            next(reader)  # Skip header\n",
    "        return sum(1 for row in reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5275c547-7f9a-45a3-9d29-b02754e0f493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_csv(input_file, percentages, output_dir=None, has_header=None, delimiter=','):\n",
    "    \"\"\"\n",
    "    Split CSV file into chunks based on percentages\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input CSV file\n",
    "        percentages (list): List of 4 percentages that sum to 100\n",
    "        output_dir (str): Output directory (default: same as input file)\n",
    "        has_header (bool): True/False/None (None = auto-detect)\n",
    "        delimiter (str): CSV delimiter\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate percentages\n",
    "    if len(percentages) != 4:\n",
    "        raise ValueError(\"Must provide exactly 4 percentages\")\n",
    "    \n",
    "    if abs(sum(percentages) - 100) > 0.01:\n",
    "        raise ValueError(f\"Percentages must sum to 100, got {sum(percentages)}\")\n",
    "    \n",
    "    # Setup paths\n",
    "    input_path = Path(input_file)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = input_path.parent\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Auto-detect header if not specified\n",
    "    if has_header is None:\n",
    "        has_header = detect_header(input_file, delimiter)\n",
    "        print(f\"Auto-detected header: {'Yes' if has_header else 'No'}\")\n",
    "    \n",
    "    # Count total data rows\n",
    "    total_rows = count_rows(input_file, has_header, delimiter)\n",
    "    print(f\"Total data rows: {total_rows}\")\n",
    "    \n",
    "    # Calculate chunk sizes\n",
    "    chunk_sizes = []\n",
    "    for i, pct in enumerate(percentages):\n",
    "        if i == 3:  # Last chunk gets remaining rows\n",
    "            chunk_size = total_rows - sum(chunk_sizes)\n",
    "        else:\n",
    "            chunk_size = int(total_rows * pct / 100)\n",
    "        chunk_sizes.append(chunk_size)\n",
    "    \n",
    "    print(f\"Chunk sizes: {chunk_sizes}\")\n",
    "    \n",
    "    # Read header if exists\n",
    "    header_row = None\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter)\n",
    "        if has_header:\n",
    "            header_row = next(reader)\n",
    "    \n",
    "    # Split the file\n",
    "    base_name = input_path.stem\n",
    "    extension = input_path.suffix\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile, delimiter=delimiter)\n",
    "        \n",
    "        # Skip header in input\n",
    "        if has_header:\n",
    "            next(reader)\n",
    "        \n",
    "        current_chunk = 0\n",
    "        current_row_count = 0\n",
    "        output_file = None\n",
    "        writer = None\n",
    "        \n",
    "        for row_num, row in enumerate(reader):\n",
    "            # Open new chunk file if needed\n",
    "            if current_row_count == 0:\n",
    "                if output_file:\n",
    "                    output_file.close()\n",
    "                \n",
    "                chunk_filename = f\"{base_name}_chunk_{current_chunk + 1}{extension}\"\n",
    "                chunk_path = output_dir / chunk_filename\n",
    "                output_file = open(chunk_path, 'w', newline='', encoding='utf-8')\n",
    "                writer = csv.writer(output_file, delimiter=delimiter)\n",
    "                \n",
    "                # Write header to each chunk if original had header\n",
    "                if has_header and header_row:\n",
    "                    writer.writerow(header_row)\n",
    "                \n",
    "                print(f\"Creating chunk {current_chunk + 1}: {chunk_filename}\")\n",
    "            \n",
    "            # Write row to current chunk\n",
    "            writer.writerow(row)\n",
    "            current_row_count += 1\n",
    "            \n",
    "            # Check if current chunk is complete\n",
    "            if current_row_count >= chunk_sizes[current_chunk]:\n",
    "                current_row_count = 0\n",
    "                current_chunk += 1\n",
    "                \n",
    "                # Stop if we've created all 4 chunks\n",
    "                if current_chunk >= 4:\n",
    "                    break\n",
    "        \n",
    "        # Close last file\n",
    "        if output_file:\n",
    "            output_file.close()\n",
    "    \n",
    "    print(f\"\\nSplit complete! Created {min(current_chunk + 1, 4)} chunks in {output_dir}\")\n",
    "    \n",
    "    # Print summary\n",
    "    for i in range(min(current_chunk + 1, 4)):\n",
    "        chunk_filename = f\"{base_name}_chunk_{i + 1}{extension}\"\n",
    "        chunk_path = output_dir / chunk_filename\n",
    "        if chunk_path.exists():\n",
    "            chunk_rows = count_rows(str(chunk_path), has_header, delimiter)\n",
    "            actual_pct = (chunk_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "            print(f\"  {chunk_filename}: {chunk_rows} rows ({actual_pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff801458-457e-40fe-af90-a583715ba521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def csv_to_json(csv_file, json_file):\n",
    "    try:\n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            data = list(reader)\n",
    "            \n",
    "        if not data:\n",
    "            print(\"No data in the file\")\n",
    "            return\n",
    "            \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "            \n",
    "        print(f\"Successfully converted {csv_file} to {json_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {csv_file} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6869d2-3250-4e5b-baeb-4ba26e6c04c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def csv_to_xml(csv_file, xml_file):\n",
    "    try:\n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            data = list(reader)\n",
    "            \n",
    "        if not data:\n",
    "            print(\"No data in the file\")\n",
    "            return\n",
    "            \n",
    "        root = ET.Element(\"data\")\n",
    "        \n",
    "        for row in data:\n",
    "            record = ET.SubElement(root, \"record\")\n",
    "            for key, value in row.items():\n",
    "                elem = ET.SubElement(record, key)\n",
    "                elem.text = value\n",
    "                \n",
    "        ET.indent(root, space=\"  \")\n",
    "        tree = ET.ElementTree(root)\n",
    "        tree.write(xml_file, encoding='utf-8', xml_declaration=True)\n",
    "        \n",
    "        print(f\"Successfully converted {csv_file} to {xml_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {csv_file} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
